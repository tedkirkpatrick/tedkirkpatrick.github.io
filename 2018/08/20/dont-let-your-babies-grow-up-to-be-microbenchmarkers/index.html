<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="description" content="Blog for A. E. Kirkpatrick and  Kirkpatrick Computing Services">
  <meta name="author" content="Arthur E. Kirkpatrick">
  <meta name="copyright" content="Arthur E. Kirkpatrick, 2018">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Don't let your babies grow up to be microbenchmarkers! &middot; All my marbles in one place
    
  </title>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66576928-4', 'auto');
  ga('send', 'pageview');
  </script>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Begin Jekyll SEO tag v2.2.3 -->
<meta property="og:title" content="Don’t let your babies grow up to be microbenchmarkers!" />
<meta name="author" content="Ted Kirkpatrick" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I never wanted to be a microbenchmarker. I was OK with a modest but respectable career choice, such as programming in RPG II or IBM 1604 assembler." />
<meta property="og:description" content="I never wanted to be a microbenchmarker. I was OK with a modest but respectable career choice, such as programming in RPG II or IBM 1604 assembler." />
<link rel="canonical" href="http://localhost:4000/2018/08/20/dont-let-your-babies-grow-up-to-be-microbenchmarkers/" />
<meta property="og:url" content="http://localhost:4000/2018/08/20/dont-let-your-babies-grow-up-to-be-microbenchmarkers/" />
<meta property="og:site_name" content="All my marbles in one place" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-08-20T00:00:00-07:00" />
<script type="application/ld+json">
{"@context":"http://schema.org","@type":"BlogPosting","headline":"Don’t let your babies grow up to be microbenchmarkers!","author":{"@type":"Person","name":"Ted Kirkpatrick"},"datePublished":"2018-08-20T00:00:00-07:00","dateModified":"2018-08-20T00:00:00-07:00","description":"I never wanted to be a microbenchmarker. I was OK with a modest but respectable career choice, such as programming in RPG II or IBM 1604 assembler.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/08/20/dont-let-your-babies-grow-up-to-be-microbenchmarkers/"},"url":"http://localhost:4000/2018/08/20/dont-let-your-babies-grow-up-to-be-microbenchmarkers/"}</script>
<!-- End Jekyll SEO tag -->

</head>


  <body class="theme-base-0c layout-reverse">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          All my marbles in one place
        </a>
      </h1>
      <p class="lead">A blog about course design, data display, C++, and Python.  Yes, these are related, at least as I see them.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/credits/">Credits</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

    
        <div class="sidebar-tags">
        Tags:<br/>
        
	   
	    <a href="/tag/C++/">C++</a>,
	  
        
	   
	    <a href="/tag/Python/">Python</a>,
	  
        
	   
	    <a href="/tag/Course design/">Course design</a>,
	  
        
	   
	    <a href="/tag/Soft skills/">Soft skills</a>,
	  
        
	   
	    <a href="/tag/Statistics/">Statistics</a>,
	  
        
	   
	    <a href="/tag/Experimental design/">Experimental design</a>,
	  
        
	  	    
            <a href="/tag/Rhetoric/">Rhetoric</a>
          
        
	</div>
      

    </nav>

    <p>&copy; 2018. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Don't let your babies grow up to be microbenchmarkers!</h1>
  <span class="post-date">20 Aug 2018
  
    <span class="tag-block">Tags: 
    
      
      <a href="/tag/C++">C++</a>
      
    
    </span>
  
  </span>
  
  <p>I never wanted to be a microbenchmarker. I was OK with a modest but
respectable career choice, such as programming in <a href="https://en.wikipedia.org/wiki/IBM_RPG_II">RPG II</a>
or <a href="http://www.computerhistory.org/collections/catalog/102635171">IBM 1604 assembler</a>.</p>

<p>But then I went and wrote several posts on the machine language efficiency of
the code compiled from several
<a href="http://kirkpatricktech.org/2017/05/03/analysis-of-code-generated-from-the-transform-idiom/">STL-based idioms</a>.
Which pretty much demanded a follow up actually comparing the performance of the various
algorithms in practice. Which in turn meant microbenchmarks. And here
we are.</p>

<h2 id="microbenchmarks-are-influenced-by-many-sources-of-variability">Microbenchmarks are influenced by many sources of variability</h2>

<p>The pitfalls of microbenchmarks are legion.  Oh, it’s easy enough to run a few
timing tests and compute some comparative measure but how broadly can you generalize
from those numbers?  There’s plenty of sources of variability underlying a
microbenchmark:</p>

<ul>
  <li><a href="http://shape-of-code.coding-guidelines.com/2015/02/24/hardware-variability-may-be-greater-than-algorithmic-improvement/">Variance between instances of the same chip design</a></li>
  <li>Variance between microarchitectures in the same family</li>
  <li>Variance between vendor implementations of a common instruction set</li>
  <li>Variance between architectures (x86 vs. ARM)</li>
  <li>Variance due to chip temperature changes, including those caused by a benchmark run</li>
  <li><a href="https://blog.acolyer.org/2017/11/07/virtual-machine-warmup-blows-hot-and-cold/">Variance across virtual machine startups</a></li>
  <li>Compiled code variance with compiler, version, and optimization level</li>
  <li>Variance due to test data sets</li>
  <li>Interference from co-resident processes and active kernel threads on the same machine</li>
  <li>Heap layout</li>
  <li>Timing of garbage collection</li>
  <li>Other factors …</li>
</ul>

<h2 id="an-example">An example</h2>

<p>Recently, I microbenchmarked two C++ implementations of a simple text
manipulation problem (the one given to Albino Tonnina for his
<a href="https://hackernoon.com/how-to-lose-an-it-job-in-10-minutes-3d63213c8370">job interview</a>).
One was an <em>O(n<sup>2</sup>)</em> iterative algorithm featuring a pair of nested loops, the other an <em>O(n)</em>
algorithm using a hash table. For my initial benchmarks, I used a
test dataset with <em>n</em>=740 and ran benchmarks using
<a href="https://nonius.io/">nonius</a>, which computes
confidence intervals for the mean and standard deviation using
the
<a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Methods_for_bootstrap_confidence_intervals">accelerated bias-corrected bootstrap</a>,
a nonparametric estimator.</p>

<p>The initial implementations performed broadly as their respective
asymptotic behaviour suggested: On this dataset, the hash table implementation was
typically about 100 times as fast as the nested-loop implementation.</p>

<p>But I wanted more than simple confirmation of their expected
asymptotic performance. As each implementation was a first draft,
I wanted to remove inefficiencies in their code and see how their
respective performance improved.  The 
<a href="https://github.com/namhyung/uftrace">uftrace</a>
user-space tracer revealed a number of extraneous
string constructor calls in the nested-loop algorithm, as well as some
superfluous calls to other expensive routines.</p>

<p>The first few performance fixes for the nested-loop implementation yielded
sufficiently large improvements that it became only
2–3 times as slow as the hash table implementation. At this
point I hit a wall, as I could no longer get repeatable benchmark
results.
Consecutive runs of
the benchmark would yield means that were never within the computed
confidence intervals of each other.</p>

<p>Some runs using nonius’s default of 100 runs (times in microseconds):</p>

<table class="table">
  <thead>
    <tr>
      <th style="text-align: right">Mean</th>
      <th style="text-align: right">Lower bound 95% CI</th>
      <th style="text-align: right">Upper bound 95% CI</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">662.5</td>
      <td style="text-align: right">660.6</td>
      <td style="text-align: right">666.0</td>
    </tr>
    <tr>
      <td style="text-align: right">675.0</td>
      <td style="text-align: right">671.9</td>
      <td style="text-align: right">680.3</td>
    </tr>
    <tr>
      <td style="text-align: right">678.5</td>
      <td style="text-align: right">669.3</td>
      <td style="text-align: right">698.9</td>
    </tr>
    <tr>
      <td style="text-align: right">742.7</td>
      <td style="text-align: right">738.0</td>
      <td style="text-align: right">749.3</td>
    </tr>
  </tbody>
</table>

<p>Of the six combinations, only two of the estimated means (678.5 lies
in 675.0’s interval, 675.0 lies in 678.5’s interval) lie within
the CI estimated for another mean.</p>

<h2 id="how-do-i-interpret-this-variation">How do I interpret this variation?</h2>

<p>The bootstrap method is powerful but requires care. It can be
sensitive to sample size (the nonius default of 100 runs may well be
too small). Running the benchmark with 1000 runs yields estimates
within the range of those above but no greater consistency of means to
confidence intervals, though the confidence intervals tend to be
tighter.</p>

<p>I am unsure how to interpret these numbers. The confidence intervals are
computed from the sample of runs and so each is subject to the particularities
of its underlying sample. If a sample run sequence includes
values from the extreme low or
high tail (and for timing data, the
probability of extremely high values is much higher than
for low values), these values will skew the computed interval. The
intervals will not necessarily overlap. Some distance between them is expected. But
how far apart is too far? How tight must the means be for us
to consider them “consistent”?</p>

<h2 id="what-are-microbenchmarks-good-for">What are microbenchmarks good for?</h2>

<p>Ultimately, the microbenchmarks are consistent enough to indicate that the nested-loop
implementation is substantially slower than the hash table
implementation for this dataset.  But they do not provide sufficiently
consistent results to say <em>how much</em> faster the hash table
implementation is. When I get such variance between benchmark runs,
which means do I present as the “results”?</p>

<p>I’ve wrestled with this topic for months, revising this post.  I don’t
have a firm conclusion.  Microbenchmarks are variable, difficult to
interpret, even unreliable, but what else do we have?  My goal was to
gain a sense of the relative performance of several C++ features, as
implemented by a specific compiler and standard library, and I
achieved that.  The microbenchmarks could distinguish substantial
performance improvements from trivial ones, a useful result.  But in
the end they were incapable of making a definitive estimate of the
sizes of those improvements.  Perhaps I should just set my
expectations to a more realistic level.</p>



</div>




    </div>

  </body>
</html>
